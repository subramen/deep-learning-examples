# Quantization
Quantization is a quick and efficient way to compress your deep learning models. This folder contains code for quantizing models

## Notebooks 
#### [Quant_101.ipynb](Quant_101.ipynb) 
Fundamentals of quantization in pure Python code. 

#### [Quant_Workflow.ipynb](Quant_Workflow.ipynb)
Step-by-step workflow when quantizing your model from 32-bit floating point to 8-bit integer

## Further Reading
* [Quantization â€” PyTorch 1.11.0 documentation](https://pytorch.org/docs/stable/quantization.html)
* [Practical Quantization in PyTorch](https://pytorch.org/blog/quantization-in-practice/)
* [FX Graph Mode Quantization User Guide](https://pytorch.org/tutorials/prototype/fx_graph_mode_quant_guide.html)
* [PyTorch Forum - Quantization](https://discuss.pytorch.org/c/quantization/17) 
* [PyTorch Github Issues](https://github.com/pytorch/pytorch/issues)

## Issues/Requests
If you encounter a bug, please open an issue or a PR.

